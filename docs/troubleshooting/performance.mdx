---
title: 'Troubleshoot: Performance'
---

## My EP doesn't seem to work!
`ort` is designed to fail gracefully when an execution provider is not available. It logs failure events through [`tracing`](https://crates.io/crates/tracing), thus you'll need a library that subscribes to `tracing` events to see the logs. The simplest way to do this is to use [`tracing-subscriber`](https://crates.io/crates/tracing-subscriber).

<Steps>
    <Step title="Add tracing-subscriber to your dependencies">
        ```toml Cargo.toml
        [dependencies]
        tracing-subscriber = { version = "0.3", features = [ "env-filter", "fmt" ] }
        ```
    </Step>
    <Step title="Initialize the subscriber in the main function">
        ```rust main.rs
        fn main() {
            tracing_subscriber::fmt::init();
        }
        ```
    </Step>
    <Step title="Show debug messages from ort">
        Set the environment variable `RUST_LOG` to `ort=debug` to see all debug messages from `ort`.
        <Tabs>
            <Tab title="Windows (PowerShell)">
                ```powershell
                $env:RUST_LOG = 'ort=debug';
                cargo run
                ```
            </Tab>
            <Tab title="Windows (Command Prompt)">
                ```cmd
                set RUST_LOG=ort=debug
                cargo run
                ```
            </Tab>
            <Tab title="Linux">
                ```shell
                RUST_LOG="ort=debug" cargo run
                ```
            </Tab>
            <Tab title="macOS">
                ```shell
                RUST_LOG="ort=debug" cargo run
                ```
            </Tab>
        </Tabs>
    </Step>
</Steps>

<Note>You can also detect EP regsitration failures programmatically. See [Execution providers: Fallback behavior](/perf/execution-providers#fallback-behavior) for more info.</Note>

## Inference is slow, even with an EP!
There are a few things you could try to improve performance:
- **Run `onnxsim` on the model.** Direct exports from PyTorch can leave a lot of junk nodes in the graph, which could hinder performance. [`onnxsim`](https://github.com/daquexian/onnx-simplifier) is a neat tool that can be used to simplify the ONNX graph and potentially improve performance.
- **Export with an older opset.** Some EPs might not support newer, more complex nodes. Try targeting an older ONNX opset when exporting your model to force it to export with simpler operations.
- **Use the [transformer optimization tool](https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/python/tools/transformers).** This is another neat tool that converts certain transformer-based models to far more optimized graphs.
- **Try other EPs.** There may be multiple EPs for your hardware that have a more performant implementation.
    - For NVIDIA, you can try CUDA, TensorRT, or DirectML.
    - For AMD, you can try ROCm, MIGraphX, or DirectML. 
    - For ARM, you can try ArmNN, ACL, or XNNPACK.
    - See [Execution providers](/perf/execution-providers) for more information on supported EPs.
- **Use [`I/O binding`](/perf/io-binding).** This can reduce latency caused by copying the session inputs/outputs to/from devices.
- **[Quantize your model.](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html)** You can try quantizing your model to 8-bit precision. This comes with a small accuracy loss, but can sometimes provide a large performance boost. If the accuracy loss is too high, you can also use [float16/mixed precision](https://onnxruntime.ai/docs/performance/model-optimizations/float16.html).
